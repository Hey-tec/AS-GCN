std: 0.227715402841568
loss: 0.9694156646728516
Epoch: 001, Train: 0.6283, Val: 0.4016, Test: 0.3292
std: 0.2069043070077896
loss: 0.5623204112052917
Epoch: 002, Train: 0.5041, Val: 0.4547, Test: 0.5555
std: 0.21253880858421326
loss: 0.4621765911579132
Epoch: 003, Train: 0.4873, Val: 0.5983, Test: 0.6707
std: 0.21010588109493256
loss: 0.5167784094810486
Epoch: 004, Train: 0.4580, Val: 0.5983, Test: 0.6708
std: 0.21107839047908783
loss: 0.5780115127563477
Epoch: 005, Train: 0.4578, Val: 0.5983, Test: 0.6708
std: 0.2097494900226593
loss: 0.576286256313324
Epoch: 006, Train: 0.4578, Val: 0.5983, Test: 0.6708
std: 0.21164578199386597
loss: 0.528384268283844
Epoch: 007, Train: 0.5217, Val: 0.5983, Test: 0.6708
std: 0.21472257375717163
loss: 0.47138121724128723
Epoch: 008, Train: 0.5769, Val: 0.5983, Test: 0.6708
std: 0.21643279492855072
loss: 0.43457186222076416
Epoch: 009, Train: 0.5947, Val: 0.5983, Test: 0.6708
std: 0.2189004272222519
loss: 0.4230818748474121
Epoch: 010, Train: 0.6271, Val: 0.5983, Test: 0.6708
std: 0.21996457874774933
loss: 0.4369133710861206
Epoch: 011, Train: 0.6282, Val: 0.5983, Test: 0.6708
std: 0.2207155078649521
loss: 0.44872429966926575
Epoch: 012, Train: 0.6283, Val: 0.5983, Test: 0.6708
std: 0.2295050472021103
loss: 0.45858195424079895
Epoch: 013, Train: 0.6279, Val: 0.5983, Test: 0.6708
std: 0.23048250377178192
loss: 0.45031601190567017
Epoch: 014, Train: 0.6278, Val: 0.5983, Test: 0.6708
std: 0.23236262798309326
loss: 0.4353134036064148
Epoch: 015, Train: 0.6278, Val: 0.5983, Test: 0.6708
std: 0.23145940899848938
loss: 0.4104800820350647
Epoch: 016, Train: 0.6268, Val: 0.5983, Test: 0.6708
std: 0.23168806731700897
loss: 0.38880041241645813
Epoch: 017, Train: 0.8036, Val: 0.7879, Test: 0.7264
std: 0.2336488962173462
loss: 0.38004961609840393
Epoch: 018, Train: 0.8489, Val: 0.8572, Test: 0.8648
std: 0.2335003912448883
loss: 0.3739503026008606
Epoch: 019, Train: 0.8260, Val: 0.8572, Test: 0.8648
std: 0.23254725337028503
loss: 0.373361736536026
Epoch: 020, Train: 0.7959, Val: 0.8572, Test: 0.8648
std: 0.23281903564929962
loss: 0.377057820558548
Epoch: 021, Train: 0.7421, Val: 0.8572, Test: 0.8648
std: 0.23136141896247864
loss: 0.3837077021598816
Epoch: 022, Train: 0.7337, Val: 0.8572, Test: 0.8648
std: 0.2308652251958847
loss: 0.37020692229270935
Epoch: 023, Train: 0.7422, Val: 0.8572, Test: 0.8648
std: 0.22991596162319183
loss: 0.36065995693206787
Epoch: 024, Train: 0.7993, Val: 0.8735, Test: 0.8990
std: 0.22951821982860565
loss: 0.3439546525478363
Epoch: 025, Train: 0.8402, Val: 0.8945, Test: 0.9045
std: 0.22739671170711517
loss: 0.3300866186618805
Epoch: 026, Train: 0.8714, Val: 0.9024, Test: 0.8854
std: 0.22597560286521912
loss: 0.32583585381507874
Epoch: 027, Train: 0.8931, Val: 0.9053, Test: 0.8790
std: 0.22505652904510498
loss: 0.3157691955566406
Epoch: 028, Train: 0.9019, Val: 0.9053, Test: 0.8790
std: 0.22309839725494385
loss: 0.3144000172615051
Epoch: 029, Train: 0.9055, Val: 0.9053, Test: 0.8790
std: 0.22242982685565948
loss: 0.3075633645057678
Epoch: 030, Train: 0.9067, Val: 0.9053, Test: 0.8790
std: 0.21907107532024384
loss: 0.30693385004997253
Epoch: 031, Train: 0.9060, Val: 0.9053, Test: 0.8790
std: 0.2175903171300888
loss: 0.3003027141094208
Epoch: 032, Train: 0.9019, Val: 0.9053, Test: 0.8790
std: 0.21610307693481445
loss: 0.2975010871887207
Epoch: 033, Train: 0.8929, Val: 0.9053, Test: 0.8790
std: 0.21526727080345154
loss: 0.2792208790779114
Epoch: 034, Train: 0.8817, Val: 0.9053, Test: 0.8790
std: 0.21595826745033264
loss: 0.27736058831214905
Epoch: 035, Train: 0.8716, Val: 0.9053, Test: 0.8790
std: 0.21549780666828156
loss: 0.2693483829498291
Epoch: 036, Train: 0.8629, Val: 0.9053, Test: 0.8790
std: 0.21544700860977173
loss: 0.2692888379096985
Epoch: 037, Train: 0.8555, Val: 0.9053, Test: 0.8790
std: 0.21641725301742554
loss: 0.2707884609699249
Epoch: 038, Train: 0.8498, Val: 0.9053, Test: 0.8790
std: 0.21686793863773346
loss: 0.2620219588279724
Epoch: 039, Train: 0.8464, Val: 0.9053, Test: 0.8790
std: 0.21623758971691132
loss: 0.26361510157585144
Epoch: 040, Train: 0.8474, Val: 0.9053, Test: 0.8790
std: 0.21519140899181366
loss: 0.2629866600036621
Epoch: 041, Train: 0.8541, Val: 0.9053, Test: 0.8790
std: 0.21482951939105988
loss: 0.2566714584827423
Epoch: 042, Train: 0.8612, Val: 0.9053, Test: 0.8790
std: 0.21556934714317322
loss: 0.2613568603992462
 Epoch: 043, Train: 0.8678, Val: 0.9053, Test: 0.8790
std: 0.21424600481987
loss: 0.2428959608078003
Epoch: 044, Train: 0.8728, Val: 0.9053, Test: 0.8790
std: 0.21465091407299042
loss: 0.24882188439369202
Epoch: 045, Train: 0.8765, Val: 0.9053, Test: 0.8790
std: 0.21456864476203918
loss: 0.2448209524154663
Epoch: 046, Train: 0.8789, Val: 0.9053, Test: 0.8790
std: 0.21333573758602142
loss: 0.2541695833206177
Epoch: 047, Train: 0.8805, Val: 0.9053, Test: 0.8790
std: 0.21255728602409363
loss: 0.2529488205909729
Epoch: 048, Train: 0.8813, Val: 0.9053, Test: 0.8790
std: 0.21186532080173492
loss: 0.24567200243473053
Epoch: 049, Train: 0.8817, Val: 0.9053, Test: 0.8790
std: 0.19540759921073914
loss: 0.24752499163150787
Epoch: 050, Train: 0.8814, Val: 0.9053, Test: 0.8790
std: 0.1949407160282135
loss: 0.24638362228870392
Epoch: 051, Train: 0.8804, Val: 0.9053, Test: 0.8790
std: 0.1950644850730896
loss: 0.2531798779964447
Epoch: 052, Train: 0.8795, Val: 0.9053, Test: 0.8790
std: 0.19490879774093628
loss: 0.2415117621421814
Epoch: 053, Train: 0.8780, Val: 0.9053, Test: 0.8790
std: 0.19476009905338287
loss: 0.24898217618465424
Epoch: 054, Train: 0.8767, Val: 0.9060, Test: 0.8827
std: 0.19292189180850983
loss: 0.2543022632598877
Epoch: 055, Train: 0.8753, Val: 0.9069, Test: 0.8854
std: 0.1936935931444168
loss: 0.24325229227542877
Epoch: 056, Train: 0.8747, Val: 0.9079, Test: 0.8874
std: 0.1930723488330841
loss: 0.24614670872688293
Epoch: 057, Train: 0.8744, Val: 0.9083, Test: 0.8883
std: 0.19269663095474243
loss: 0.24290740489959717
Epoch: 058, Train: 0.8746, Val: 0.9086, Test: 0.8884
std: 0.19162416458129883
loss: 0.24659892916679382
Epoch: 059, Train: 0.8743, Val: 0.9089, Test: 0.8892
std: 0.1916467845439911
loss: 0.24187353253364563
Epoch: 060, Train: 0.8748, Val: 0.9090, Test: 0.8885
std: 0.19212961196899414
loss: 0.24108509719371796
Epoch: 061, Train: 0.8750, Val: 0.9090, Test: 0.8885
std: 0.1924649477005005
loss: 0.24691523611545563
Epoch: 062, Train: 0.8755, Val: 0.9090, Test: 0.8885
std: 0.19078263640403748
loss: 0.24648666381835938
Epoch: 063, Train: 0.8756, Val: 0.9090, Test: 0.8885
std: 0.19040431082248688
loss: 0.2500776946544647
Epoch: 064, Train: 0.8752, Val: 0.9090, Test: 0.8885
std: 0.1914631724357605
loss: 0.24676336348056793
Epoch: 065, Train: 0.8752, Val: 0.9090, Test: 0.8885
std: 0.1910371482372284
loss: 0.24343842267990112
Epoch: 066, Train: 0.8737, Val: 0.9090, Test: 0.8885
std: 0.19157946109771729
loss: 0.23779229819774628
Epoch: 067, Train: 0.8723, Val: 0.9098, Test: 0.8950
std: 0.19129405915737152
loss: 0.23876118659973145
Epoch: 068, Train: 0.8712, Val: 0.9101, Test: 0.8982
std: 0.19074803590774536
loss: 0.2518593668937683
Epoch: 069, Train: 0.8700, Val: 0.9101, Test: 0.8982
std: 0.19037488102912903
loss: 0.2400132268667221
Epoch: 070, Train: 0.8693, Val: 0.9101, Test: 0.8982
std: 0.18942506611347198
loss: 0.23791593313217163
Epoch: 071, Train: 0.8694, Val: 0.9102, Test: 0.9044
std: 0.19099673628807068
loss: 0.23919600248336792
Epoch: 072, Train: 0.8701, Val: 0.9111, Test: 0.9038
std: 0.19081005454063416
loss: 0.22859594225883484
Epoch: 073, Train: 0.8711, Val: 0.9111, Test: 0.9038
std: 0.1910451203584671
loss: 0.23558512330055237
Epoch: 074, Train: 0.8722, Val: 0.9114, Test: 0.8996
std: 0.19020885229110718
loss: 0.2421952188014984
Epoch: 075, Train: 0.8727, Val: 0.9114, Test: 0.8994
std: 0.19120372831821442
loss: 0.23278646171092987
Epoch: 076, Train: 0.8733, Val: 0.9116, Test: 0.8990
std: 0.18963436782360077
loss: 0.2394380122423172
Epoch: 077, Train: 0.8737, Val: 0.9118, Test: 0.8995
std: 0.1898770034313202
loss: 0.23946954309940338
Epoch: 078, Train: 0.8727, Val: 0.9118, Test: 0.8995
std: 0.19105511903762817
loss: 0.23415599763393402
Epoch: 079, Train: 0.8717, Val: 0.9118, Test: 0.8995
std: 0.19039829075336456
loss: 0.2405441850423813
Epoch: 080, Train: 0.8704, Val: 0.9118, Test: 0.8995
std: 0.18977084755897522
loss: 0.2316763699054718
Epoch: 081, Train: 0.8698, Val: 0.9118, Test: 0.8995
std: 0.19235014915466309
loss: 0.2326868772506714
Epoch: 082, Train: 0.8700, Val: 0.9118, Test: 0.8995
std: 0.1911478191614151
loss: 0.22599822282791138
Epoch: 083, Train: 0.8711, Val: 0.9118, Test: 0.8995
std: 0.18979398906230927
loss: 0.22014527022838593
Epoch: 084, Train: 0.8729, Val: 0.9121, Test: 0.9047
std: 0.19209936261177063
loss: 0.22744596004486084
Epoch: 085, Train: 0.8767, Val: 0.9133, Test: 0.9030
std: 0.19217826426029205
loss: 0.2298843115568161
Epoch: 086, Train: 0.8812, Val: 0.9135, Test: 0.9006
std: 0.19116215407848358
loss: 0.23809464275836945
Epoch: 087, Train: 0.8836, Val: 0.9147, Test: 0.8988
std: 0.1923777461051941
loss: 0.23001807928085327
Epoch: 088, Train: 0.8851, Val: 0.9147, Test: 0.8970
std: 0.19217461347579956
loss: 0.23695343732833862
Epoch: 089, Train: 0.8847, Val: 0.9152, Test: 0.8971
std: 0.19241666793823242
loss: 0.23483125865459442
Epoch: 090, Train: 0.8833, Val: 0.9158, Test: 0.8988
std: 0.19293305277824402
loss: 0.2392813265323639
Epoch: 091, Train: 0.8817, Val: 0.9159, Test: 0.9004
std: 0.19209343194961548
loss: 0.22915416955947876
Epoch: 092, Train: 0.8802, Val: 0.9159, Test: 0.9004
std: 0.1930476427078247
loss: 0.22878937423229218
Epoch: 093, Train: 0.8794, Val: 0.9160, Test: 0.9031
std: 0.19333043694496155
loss: 0.23173588514328003
Epoch: 094, Train: 0.8804, Val: 0.9160, Test: 0.9031
std: 0.19227895140647888
loss: 0.2321697473526001
Epoch: 095, Train: 0.8810, Val: 0.9160, Test: 0.9031
std: 0.19311043620109558
loss: 0.23683221638202667
Epoch: 096, Train: 0.8804, Val: 0.9160, Test: 0.9031
std: 0.19354449212551117
loss: 0.2280271351337433
Epoch: 097, Train: 0.8802, Val: 0.9160, Test: 0.9031
std: 0.19443227350711823
loss: 0.23332881927490234
Epoch: 098, Train: 0.8807, Val: 0.9160, Test: 0.9031
std: 0.1945178210735321
loss: 0.23230114579200745
Epoch: 099, Train: 0.8807, Val: 0.9160, Test: 0.9031
std: 0.1957351565361023
loss: 0.2381543666124344
Epoch: 100, Train: 0.8806, Val: 0.9160, Test: 0.9031
std: 0.19458025693893433
loss: 0.22871139645576477
Epoch: 101, Train: 0.8811, Val: 0.9160, Test: 0.9031
std: 0.1955629140138626
loss: 0.2222200185060501
Epoch: 102, Train: 0.8811, Val: 0.9160, Test: 0.9031
std: 0.19479027390480042
loss: 0.22014424204826355
Epoch: 103, Train: 0.8809, Val: 0.9160, Test: 0.9031
std: 0.19592155516147614
loss: 0.2382991760969162
Epoch: 104, Train: 0.8779, Val: 0.9160, Test: 0.9031
std: 0.19439886510372162
loss: 0.23931469023227692
Epoch: 105, Train: 0.8756, Val: 0.9160, Test: 0.9031
std: 0.19551104307174683
loss: 0.21867871284484863
Epoch: 106, Train: 0.8754, Val: 0.9160, Test: 0.9031
std: 0.19661633670330048
loss: 0.22266075015068054
Epoch: 107, Train: 0.8753, Val: 0.9160, Test: 0.9031
std: 0.19752928614616394
loss: 0.22141647338867188
Epoch: 108, Train: 0.8768, Val: 0.9160, Test: 0.9031
std: 0.19831959903240204
loss: 0.2212468534708023
Epoch: 109, Train: 0.8796, Val: 0.9160, Test: 0.9031
std: 0.19819428026676178
loss: 0.22866281867027283
Epoch: 110, Train: 0.8790, Val: 0.9160, Test: 0.9031
std: 0.1955515444278717
loss: 0.22808656096458435
Epoch: 111, Train: 0.8780, Val: 0.9160, Test: 0.9031
std: 0.19641312956809998
loss: 0.22764475643634796
Epoch: 112, Train: 0.8775, Val: 0.9160, Test: 0.9031
std: 0.19600613415241241
loss: 0.21956567466259003
Epoch: 113, Train: 0.8759, Val: 0.9160, Test: 0.9031
std: 0.19535213708877563
loss: 0.22549694776535034
Epoch: 114, Train: 0.8753, Val: 0.9160, Test: 0.9031
std: 0.19492727518081665
loss: 0.22884094715118408
Epoch: 115, Train: 0.8780, Val: 0.9160, Test: 0.9075
std: 0.19516624510288239
loss: 0.21813905239105225
Epoch: 116, Train: 0.8805, Val: 0.9165, Test: 0.9052
std: 0.19444845616817474
loss: 0.22345194220542908
Epoch: 117, Train: 0.8774, Val: 0.9165, Test: 0.9052
std: 0.19545219838619232
loss: 0.22595566511154175
Epoch: 118, Train: 0.8744, Val: 0.9165, Test: 0.9052
std: 0.19262458384037018
loss: 0.2201279252767563
Epoch: 119, Train: 0.8716, Val: 0.9165, Test: 0.9052
std: 0.19356335699558258
loss: 0.22277064621448517
Epoch: 120, Train: 0.8715, Val: 0.9165, Test: 0.9052
std: 0.19362366199493408
loss: 0.2326323688030243
Epoch: 121, Train: 0.8715, Val: 0.9165, Test: 0.9052
std: 0.19428454339504242
loss: 0.222451850771904
Epoch: 122, Train: 0.8727, Val: 0.9165, Test: 0.9052
std: 0.19444938004016876
loss: 0.22245760262012482
Epoch: 123, Train: 0.8734, Val: 0.9165, Test: 0.9052
std: 0.19430199265480042
loss: 0.2141825407743454
Epoch: 124, Train: 0.8739, Val: 0.9165, Test: 0.9052
std: 0.19510217010974884
loss: 0.22811520099639893
Epoch: 125, Train: 0.8741, Val: 0.9165, Test: 0.9052
std: 0.19740653038024902
loss: 0.21966812014579773
Epoch: 126, Train: 0.8754, Val: 0.9165, Test: 0.9052
std: 0.19816213846206665
loss: 0.22557833790779114
Epoch: 127, Train: 0.8770, Val: 0.9169, Test: 0.9108
std: 0.19758307933807373
loss: 0.2233871966600418
Epoch: 128, Train: 0.8775, Val: 0.9169, Test: 0.9108
std: 0.1993589848279953
loss: 0.2245151549577713
Epoch: 129, Train: 0.8780, Val: 0.9171, Test: 0.9100
std: 0.19959071278572083
loss: 0.22929935157299042
Epoch: 130, Train: 0.8784, Val: 0.9171, Test: 0.9100
std: 0.19987612962722778
loss: 0.22740019857883453
Epoch: 131, Train: 0.8765, Val: 0.9171, Test: 0.9100
std: 0.19932983815670013
loss: 0.22601336240768433
Epoch: 132, Train: 0.8730, Val: 0.9171, Test: 0.9100
std: 0.1977853626012802
loss: 0.21877047419548035
Epoch: 133, Train: 0.8718, Val: 0.9171, Test: 0.9100
std: 0.19716106355190277
loss: 0.21294327080249786
Epoch: 134, Train: 0.8730, Val: 0.9171, Test: 0.9100
std: 0.19626927375793457
loss: 0.22420601546764374
Epoch: 135, Train: 0.8727, Val: 0.9171, Test: 0.9100
std: 0.19700248539447784
loss: 0.2186763733625412
Epoch: 136, Train: 0.8745, Val: 0.9171, Test: 0.9100
std: 0.19643375277519226
loss: 0.2275080382823944
Epoch: 137, Train: 0.8771, Val: 0.9171, Test: 0.9100
std: 0.1968156397342682
loss: 0.2257102131843567
Epoch: 138, Train: 0.8796, Val: 0.9171, Test: 0.9100
std: 0.19835993647575378
loss: 0.229589581489563
Epoch: 139, Train: 0.8826, Val: 0.9171, Test: 0.9100
std: 0.1979319453239441
loss: 0.22905586659908295
Epoch: 140, Train: 0.8846, Val: 0.9171, Test: 0.9100
std: 0.19810205698013306
loss: 0.22549211978912354
Epoch: 141, Train: 0.8862, Val: 0.9171, Test: 0.9100
std: 0.19909141957759857
loss: 0.2239833027124405
Epoch: 142, Train: 0.8873, Val: 0.9171, Test: 0.9100
std: 0.19901308417320251
loss: 0.22278745472431183
Epoch: 143, Train: 0.8872, Val: 0.9171, Test: 0.9100
std: 0.19880224764347076
loss: 0.23684705793857574
Epoch: 144, Train: 0.8863, Val: 0.9171, Test: 0.9040
std: 0.19866888225078583
loss: 0.21531140804290771
Epoch: 145, Train: 0.8849, Val: 0.9173, Test: 0.9062
std: 0.19751255214214325
loss: 0.2195313721895218
Epoch: 146, Train: 0.8828, Val: 0.9173, Test: 0.9062
std: 0.19754333794116974
loss: 0.23204246163368225
Epoch: 147, Train: 0.8797, Val: 0.9173, Test: 0.9062
std: 0.1970963031053543
loss: 0.2198047637939453
Epoch: 148, Train: 0.8773, Val: 0.9173, Test: 0.9062
std: 0.19711022078990936
loss: 0.21857352554798126
Epoch: 149, Train: 0.8766, Val: 0.9173, Test: 0.9062
std: 0.19799968600273132
loss: 0.21716539561748505
Epoch: 150, Train: 0.8762, Val: 0.9173, Test: 0.9062
std: 0.19861719012260437
loss: 0.22550544142723083
Epoch: 151, Train: 0.8740, Val: 0.9173, Test: 0.9062
std: 0.19799727201461792
loss: 0.2305680513381958
Epoch: 152, Train: 0.8710, Val: 0.9173, Test: 0.9062
std: 0.19818763434886932
loss: 0.22186370193958282
Epoch: 153, Train: 0.8712, Val: 0.9173, Test: 0.9062
std: 0.1956406533718109
loss: 0.2233046442270279
Epoch: 154, Train: 0.8688, Val: 0.9173, Test: 0.9062
std: 0.1976243257522583
loss: 0.2346169501543045
Epoch: 155, Train: 0.8665, Val: 0.9173, Test: 0.9062
std: 0.1971229910850525
loss: 0.2250467985868454
Epoch: 156, Train: 0.8698, Val: 0.9173, Test: 0.9062
std: 0.19679345190525055
loss: 0.2241889387369156
Epoch: 157, Train: 0.8751, Val: 0.9173, Test: 0.9062
std: 0.19823545217514038
loss: 0.2277190387248993
Epoch: 158, Train: 0.8792, Val: 0.9173, Test: 0.9062
std: 0.19763456284999847
loss: 0.2147735059261322
Epoch: 159, Train: 0.8822, Val: 0.9173, Test: 0.9062
std: 0.19761963188648224
loss: 0.21788614988327026
Epoch: 160, Train: 0.8838, Val: 0.9173, Test: 0.9062
std: 0.19910971820354462
loss: 0.22220128774642944
Epoch: 161, Train: 0.8858, Val: 0.9173, Test: 0.9062
std: 0.19820564985275269
loss: 0.21958445012569427
Epoch: 162, Train: 0.8863, Val: 0.9173, Test: 0.9062
std: 0.19955743849277496
loss: 0.22702038288116455
Epoch: 163, Train: 0.8840, Val: 0.9173, Test: 0.9062
std: 0.19881625473499298
loss: 0.23650547862052917
Epoch: 164, Train: 0.8793, Val: 0.9173, Test: 0.9062
std: 0.19946788251399994
loss: 0.22111935913562775
Epoch: 165, Train: 0.8738, Val: 0.9173, Test: 0.9062
std: 0.1977667212486267
loss: 0.22274863719940186
Epoch: 166, Train: 0.8706, Val: 0.9173, Test: 0.9062
std: 0.1967226266860962
loss: 0.2165977954864502
Epoch: 167, Train: 0.8690, Val: 0.9173, Test: 0.9062
std: 0.19414165616035461
loss: 0.22103700041770935
Epoch: 168, Train: 0.8698, Val: 0.9173, Test: 0.9062
std: 0.19566477835178375
loss: 0.22959373891353607
Epoch: 169, Train: 0.8687, Val: 0.9173, Test: 0.9062
std: 0.19616618752479553
loss: 0.22609756886959076
Epoch: 170, Train: 0.8700, Val: 0.9173, Test: 0.9062
std: 0.19585372507572174
loss: 0.2167840301990509
Epoch: 171, Train: 0.8716, Val: 0.9173, Test: 0.9062
std: 0.19857217371463776
loss: 0.23226158320903778
Epoch: 172, Train: 0.8752, Val: 0.9173, Test: 0.9062
std: 0.19973422586917877
loss: 0.2250557839870453
Epoch: 173, Train: 0.8784, Val: 0.9173, Test: 0.9062
std: 0.19868382811546326
loss: 0.22631946206092834
Epoch: 174, Train: 0.8800, Val: 0.9173, Test: 0.9062
std: 0.1986873596906662
loss: 0.21507681906223297
Epoch: 175, Train: 0.8794, Val: 0.9173, Test: 0.9062
std: 0.1993131786584854
loss: 0.2195352464914322
Epoch: 176, Train: 0.8797, Val: 0.9173, Test: 0.9062
std: 0.19884641468524933
loss: 0.22844353318214417
Epoch: 177, Train: 0.8802, Val: 0.9173, Test: 0.9062
std: 0.1994827687740326
loss: 0.22223320603370667
Epoch: 178, Train: 0.8804, Val: 0.9173, Test: 0.9062
std: 0.19877777993679047
loss: 0.22294273972511292
Epoch: 179, Train: 0.8803, Val: 0.9173, Test: 0.9062
std: 0.19935762882232666
loss: 0.22887761890888214
Epoch: 180, Train: 0.8788, Val: 0.9173, Test: 0.9062
std: 0.19831939041614532
loss: 0.21712110936641693
Epoch: 181, Train: 0.8769, Val: 0.9173, Test: 0.9062
std: 0.19865348935127258
loss: 0.2364763617515564
Epoch: 182, Train: 0.8747, Val: 0.9173, Test: 0.9062
std: 0.19794967770576477
loss: 0.2212485820055008
Epoch: 183, Train: 0.8744, Val: 0.9173, Test: 0.9062
std: 0.19702978432178497
loss: 0.22480319440364838
Epoch: 184, Train: 0.8767, Val: 0.9173, Test: 0.9062
std: 0.19710074365139008
loss: 0.21741493046283722
Epoch: 185, Train: 0.8778, Val: 0.9173, Test: 0.9062
std: 0.19808971881866455
loss: 0.21247123181819916
Epoch: 186, Train: 0.8797, Val: 0.9173, Test: 0.9062
std: 0.19822433590888977
loss: 0.2301478534936905
Epoch: 187, Train: 0.8815, Val: 0.9173, Test: 0.9062
std: 0.19968223571777344
loss: 0.2275976687669754
Epoch: 188, Train: 0.8781, Val: 0.9173, Test: 0.9062
std: 0.19645121693611145
loss: 0.23543071746826172
Epoch: 189, Train: 0.8704, Val: 0.9173, Test: 0.9062
std: 0.19612546265125275
loss: 0.22890929877758026
Epoch: 190, Train: 0.8601, Val: 0.9173, Test: 0.9062
std: 0.19633528590202332
loss: 0.22131437063217163
Epoch: 191, Train: 0.8550, Val: 0.9173, Test: 0.9062
std: 0.19549500942230225
loss: 0.24115467071533203
Epoch: 192, Train: 0.8574, Val: 0.9173, Test: 0.9062
std: 0.19577577710151672
loss: 0.22731806337833405
Epoch: 193, Train: 0.8623, Val: 0.9173, Test: 0.9062
std: 0.1976732611656189
loss: 0.21146441996097565
Epoch: 194, Train: 0.8681, Val: 0.9173, Test: 0.9062
std: 0.19553209841251373
loss: 0.22180691361427307
Epoch: 195, Train: 0.8745, Val: 0.9173, Test: 0.9062
std: 0.1965131014585495
loss: 0.23386190831661224
Epoch: 196, Train: 0.8739, Val: 0.9173, Test: 0.9062
std: 0.19772695004940033
loss: 0.2198677957057953
Epoch: 197, Train: 0.8731, Val: 0.9173, Test: 0.9062
std: 0.19663576781749725
loss: 0.23813605308532715
Epoch: 198, Train: 0.8700, Val: 0.9173, Test: 0.9062
std: 0.19432704150676727
loss: 0.2292776256799698
Epoch: 199, Train: 0.8665, Val: 0.9173, Test: 0.9062
std: 0.19435685873031616
loss: 0.21304039657115936
Epoch: 200, Train: 0.8652, Val: 0.9173, Test: 0.9062